---

layout: "post"
title: "Review: Style Transfer"
date: 2019-07-01 10:00:00
toc: true
---

**Contents**
* TOC
{:toc}

__Style transfer__ refers to a class of methods that transfer the "look and feel" of a reference style to some images or videos. Here the definition of "look and feel" is somewhat flexible. It can be the brushstrokes and the pigments of a painterly image[^TheStarryNight], or the exquisite textures compose a modern art[^No5], or the harmonic color scheme of a landscape photo[^DeepPhotoStyleTransfer], or even the funny movement of a dancing person[^EverybodyDanceNow].

As diverse as the style can be, the "content" of the source image or video has to be preserved, such that the identity of the object or person has to be kept unchanged even its appearance has been "re-rendered".


Style transfer is right at the convergent field of computer graphics, computer vision and machine learning. It is the perfect combination of science and art: A full stack understanding of visual data is required to analysis the problem and synthesis convicing results. Once works, it can produce unmatched amount of visual joy. It covers a broad range of topics: image processing[^ImageProcessing], features extraction and analysis[^FeatureDetection], segmentation[^Segmentation], texture synthesis[^TextureSynthesis], parameteric[^ParametricTextureModel] and non-parametric[^NonParametricSampling] models, optimization[^TextureOptimization], and of course nowadays' red-hot machine topics such as autoencoders[^Autoencoder], generative models[^GenerativeModels], unsupervised learning with GANs[^GAN] etc. 

This article aims to give you an comprehensive overview on this exciting research topic. The content is structured by the timeline of research development: We will first review the early work in this area, which are computer graphics methods that emulated artistic materials. As visually plausible as the results are, these methods are rendering techniques that required significant amount of human intervention. We then fastforward to the early 2000s, where new techniques were invented by combining computer vision with computer graphics, and fully automatic example-based image synthesis started to become popular. Techniques in these period laid the foudation of nowadays' style transfer techniques. We will focus on the problems of color image editing, non-photorealistic rendering and texture synthesis, because many of today's techniques can be traced back to these ideas. Having understood the past techniques, including their strength and limitation, we will dive into the recent development in the deep learning context. Like many fields in computer science, the landscape of style transfer technique has been completely reshaped by deep learning in the past few years (As the writing of this article, I meant 2016 - 2018). We will have a journey through the seminar works in this period and understand why we are able to see what we see today in style transfer.

So buckle up, let's begin the magic carpet ride of style transfer!

#### At The Beginning, There Was Computer Graphics

Early style transfer researches focused on developing computer graphics algorithms that emulate particular media in art works. For example, hairy burshes[^HairyBrushes], oil paint[^FastPaintTexture], watercolor[^WaterColor], curved brush strokes[^CurvedBrushStroke], Impressionism[^Impressionist], pen-and-ink[^PenInk]. One good example is the _Computer-Generated Watercolor_[^WaterColor] method invented by C J. Curtis et al, which use fluid simulation to generate interaction between the pigment, the water and the paper.


One common problem for these emulation based methods is their slow speed. In particular, it is non-trivial to parellal these methods for realtime rendering. Luckily, compute graphics is all about producing visually plausile effect in timely affordable ways. If this is the end goal, there is room for deviating from realworld physics and "hack" effects that looks good on screen. One good example is the _Fast Paint Texture_[^FastPaintTexture] method developed by A. Hertzmann. Instead of simulating how oil interact with the canvas, simple textured height map and bump-mapping are used to produce a convincing oil-paint stroke effect. This "trick" allows brush strokes to be synthesized at realtime for human iteraction. 

Another common problem is the requirement of human intervention, which made these method infeasible for automatic style transfer. A good example is the _Paint by numbers_[^PaintByNumber] method developed by P. Haeherli, which requires user to select the location of each brush as well as its shape, direction and size. Several follow-up works[^PenInk] made the iteractive process semi-automated, for example, by orienting the brushes based on the underlying image gradients. However, fully automatic style transfer can not be achieved without sophisticated understanding of the image. In particular, emulation based methods can not answer the question how to seperate the content from the style, nor how to combine the style with a new content.

Beyond time complexity and automanous, it is also very expensive to design algorithms for every new style. In general, formulating mathmatical definition for the "look and feel" is an very hard problem. The field needs a universal way to automatically capture arbitary styles.

In summary, there are the three mian unsolved problem in the early days style transfer: _speed_, _automation_ and _generalization_. Since then, the field experienced two major revolutions. First time in the early 2000s, triggered by the advancement of computer vision techniques. We are now at its second revolution that triggerd by the renaissance of machine learning began at 2011.



#### First Revolution: Computer Vision 

Computer vison is about extracting useful information from visual data -- a mapping from image to model. This is opposite to computer graphics, which models the mapping from data into images. Around the early 2000s, researchers started to realize that by combining vision and graphics, one can have a complete pipeline that first extract useful information from an image, then use the information to control the generation of new images. This technique is called "example based rendering", and it revolutionized the field of style transfer. 

Among the numerous researches carried out in this period, we pick three most relevant catetories to discuss: color image transfer, non-photorealistic rendering and texture synthesis. These methods have stronge connection to the techinques we are developing today. By knowing the past, we can understand the origin of the story and the principle ideas that used to driven the field forward. By knowing the past, we will also understand the limitation of these classic ideas and the significance of the work we do today. 

__Colour Image Transfer__ Some of us have the experience of the color editing tools in programs like GIMP or Photoshop.
> "One of the most common tasks in image processing is to alter an image’s color." -- Color Transfer between Images[^ColorTransferBetweenImages], E. Reinhard et al.

Sometimes it can be tedious to figure out the best color, brightness and saturation of a photo. One of the earlist style transfer methods[^ColorTransferBetweenImages] is invented to address this problem: why not just select a reference photo that look good, and transfer its color scheme to the photo that needs editing. The colour scheme of an image is modelled by three Gaussian distributions, one for each image channel. To transfer the color scheme from one image (target) to another (source), they simply re-normalized the Gaussian distributions of the source image so them matched the ones in the target image. Notice that the normalization is performed independently to each of the three channels. For this reason, it is helpful to de-correlate the colour channels before normalization (map the RBG values into the $$l\alpha\beta$$ space). Formally, let $$[l_{s}, \alpha_{s}, \beta_{s}]$$ and $$[\sigma_{s}^{l}, \sigma_{s}^{\alpha}, \sigma_{s}^{\beta}]$$ denote the mean and standard deviation of the source image's three channels, and $$[l_{t}, \alpha_{t}, \beta_{t}]$$ and $$[\sigma_{t}^{l}, \sigma_{t}^{\alpha}, \sigma_{t}^{\beta}]$$ denote the mean and standard deviation of the target image. The transfer is applied as such:

$$l'=(l - l_{s})\frac{\sigma_{t}^l}{\sigma_{s}^l} + l_{t}, \ \ \ \alpha'=(\alpha - \alpha_{s})\frac{\sigma_{t}^\alpha}{\sigma_{s}^\alpha} + \alpha_{t}, \ \ \ \beta'=(\beta - \beta_{s})\frac{\sigma_{t}^\beta}{\sigma_{s}^\beta} + \beta_{t}$$

One interesting discovery of this study is that global transfermations in the colour feature space (either RBG or $$l\alpha\beta$$) preserve the image content very well. We will see this phenomenon, together with the ideas of feature decorrelation and Gaussian parameterization, also thrive in later deep leanring based methods[^NeuralStyleGatys],[^WCT]. 


__Non-Photorealistic Rendering__
Non-photorealistic rendering (NPR) refers to a class of rendering techniques that pursues stylization over realism. Here we only briefly discuss the major categories in this field. Please refer to this comprehensive survey paper[^STARNPR] to know all the state of the art NPR techiques before the deep learning renaissance.


Image processing, especially local filtering techniques, provide an promising alternative due to parallelizing these methods are straight forward. For many years, researchers handcrafted better and better filtering techniques. One of the seminar work in this catergory is Video Abstraction by Winnemöller et al.[^VideoAbstracion]. Their pipeline first use the bilateral filtering to perform edge-aware image smoothing, then   difference of Gaussians (DoG) filters is used to detect salient edges. The smoothed image and the edge map is combined and quantized to achive stronger effect. The downside of this category of work is it is difficult to extend the visual effect beyong the cartoon-like style, due to the limitation of handcrafted low level image filters. Luckily, as we will see, this limitation will be lifted by learning based method and deep neural networks' cascaded nonlinear processing.

Apart from image filtering, segmentation[^MeanShift] also helped non-photorealistic rendering. D. DeCarlo et al.[^StylizationDeCarlo] first segment images in a scale-space pyramid. The salient regions in the image are locally decomposed into finer segments traversing the pyramid. Outlines of the regions are smoothed and superimposed to delineate region boundaries. The result is a line-drawing style using bold edges and large regions of constant color. Segmentation method has also been used to improve temporal coherence[^VideoTooning],[^StrokeSurface] in video style transfer, or producing unique style that is composed of abstract shapes[^ArtyShape].


One NPR researcher that is worth special mention is A. Hertzmann. Among many of his highly cited publications[^ImageAnalogies],[^CurvedBrushStroke], Paint By Relaxation[^Relaxation] is the first method that frames style transfer as an optimization problem. In particular, this work formulate a combination of multiple energies to strike a good trade off between the content and the style: the desire to closely match the appearance of the source image (content preservation), and the desire to use as few strokes as possible (stronger abstraction, more stylished). With retrospect, this is significant as the first working proof of optimization for style transfer.

Unfortunately, these methods are not versatile in terms of depicting styles. They produce fantastic results for oil paintings and cartoon-like style, but are difficult to generalize to a wide variety of styles. 

__Texture Synthesis__

Texture synthesis is a widely used technique in computer graphics. The Markovian property of texture make it possible to to algorithmically constructing a large digital image from a small digital sample image. In the context of style transfer, texture synthesis is significantly more flexible than handcrafted NPR methods: the "style" is encoded in the target example and does not need any handcraft.

We have the seminal works from A. Efros. et al[^ImageQuilting] and A. Hertzmann et al.[^ImageAnalogies]. Both of these works are published at Siggraph 2001, and address the problem of example based texture transfer: rendering an image with a texture taken from a reference style image. The key idea to find correspondence between the source image and target image, and copy the best matched content (pixel or patch of pixels) from the target image to the source image. The copied content are either blend or stitched to form the final image.

One of the key challenge is how to find good correspondence between two very different images. Both of these two method proposed to match the low-passed filtered image, with the goal to reduce the influence of texture and make the match ground more on context. However, such pixel-based matching can still be unreliable.

Another key challenge is how to blend the copied content into an image. Simple overwriting or averaging overlapped pixels lead to artifacts such as ghosting and blur. A better way is to stitch patches together using boundary cut (dynamic programming[^ImageQuilting] or graphcut[^GraphCut]). This allow smooth transition between the boundary of different patches. However, it is not gaurantee that the optimial cut always lead to good result. In many case, it is simply impossible to have solution that is visually seamless. 


Expect for such limitations in pixel space, these style transfer methods still produced some mind-blowing results. They are very strong baseline to compare with the recent deep learning based methods. Many of today's texture transfer can be seen as extentions of these ideas to deep learnig domain. As we will discuss in details, with deep features, both patch matching and patch blending are more reliable. 


#### Second Revolution: Deep Learning

Many of us know about the story how AlexNet made deep learning famous almost in one night by its stylish win of 2011's ImageNet challenge. But fewer knows the first killer App of deep learning was actually happend a lot earlier than that. It is the Hinton and his students' 2009 work in Speech Recognition showed the world the first time that with enough data and enough computation resource. Since then, it was just the matter of time for deep learning to reshape the landscape of many research fields. Style transfer was no exception. 

Before diving into the (deep) learning based method, let us do a little summary for the journey so far:
1. On the bright side, we witness the value of both parametric (color image transfer) and non-parametric (texture transfer) model for style transfer. The parametric model, for example, fitting Gaussian distributions to pixel values, is a simple but effective way for transfering colour. The non-parametric model, such as the Markov Random Fields, has found its value in texture transfer. Image filtering has great success in rendering a paricular cartoon-like style. We will see all these idea will continue to shine in the deep learning era.
2. On the other side, let see what went wrong. What went wrong was all these method are excuted in pixel space (individual pixels, local patch of collection of pixels, distribution of pixel value in colour space). Such a low level representation has led to the major limitation of pre deep learning based style transfer techniques. Precisely, pixel-based representation can't not capture complex image statistics, let alone the semantics. Hence the model built upon such representation can't not fully capture neither the content nor the style.
3. In the meantime, there is lack of learning process in all these methods. They are either handcrafted (image filters) or single example based method (color transfer, texture transfer). The reason might just been it is difficult to learn anything powerful in the pixel space.

Having these observation in mind, let's continue the journey into the land of deep learning. Which in essence solves the representation problem and the learning problem.


__Deep Visualization__ The main reason that deep features are better than pixels is that they encode more useful image statistics at multiple scales. Remember the days when image filters are handcrafted? Learning these filters from data is a million times more powerful and efficient.

Deep learning and image generation was first bridged by visualization techniques [^VisualizeZeiler],[^UnderstandDeep],[^SynthesisPreferredInput],[^BuildingBlockInterpretability] that were originally invented to make the network more interpeatable. One particular type of visualization method uses backpropagation to "reconstruct" an input image from its deep feature encoding. Notice in this case the weights of the networks are fronzen, only the feature activation are modified by the backpropagtion so in the end the input image is updated.

For example, it can generate an image that would give high probability of a particular object class (the final output of the neural networks). It can also generate an image that leads to activation in intermedate layers. Such a general framework for inverting an representation paved the way for deep feature based image synthesis -- whenever you have an deep representation, geting an image out of it is straight forward.


In the context of stylye transfer, the first milestone visualization of deep features is probably Deep Dream[^DeepDream]. It starts off using the backpropagation inverting technique[^UnderstandDeep], but add a smart twist to it: instead of simply inverting the feature activations, it acturally amplifies them. That means, wheneven the neural network seem an interesting pattern or object, the input image is changed in the way that the neural network can see more of it. Formally, this is achieved by use the feature activation as the gradient to backpropagation. The results of Deep Dream can be seem as one pariticular type of style transfer, where the style are the image statistics encode in the network's weights. This leads to very intensive stylization results, almost feels like the dreams made by the networks.

One limitation of Deep Dream is the lack of controllability. Although it is possible to manually specified what filters to amplify, transfer a particular style, such as one of Van Gough's paintings, is not possible. To do that needs non-trivial reverse engineering the style in deep feature space. Our next stop are such techniques that formulate style objective using deep features and use such objective to drive the representation invertion towards a stylished image.  


__Parametric Neural Style Transfer__

Remember the color image transfer[^ColorTransferBetweenImages] which parameterises per-channel Gaussian distributions to represent the color scheme in an image, parametric methods are invented to generalize such techniques to deep feature space for style transfer. In particular, Gatys et al's 2016 was the first to sucessfully implement this idea. Their seminar paper[^NeuralStyleGatys] coined the term "neural style transfer". Since then, there have been hunders, if not more, followup works. All these works can be roughly categorized into three divisions based on the different aspects of style transfer they improved: quality, speed, and versatility.

__Quality__: Let's begin with Gatys et al.[^NeuralStyleGatys]'s seminar paper. The key insight of this work is the formulation of style as the distribution of deep feature activations.  The problem of style transfer then boils down to aligning such distribution computed from a source image to the distribution computed from a target distribution. 

Precisely, the gram matrix of feature activation as a certain layer is used to characterize the style (mainly textures) at a particular scale. Multiple gram metrices from different layers capture the style at multiple scales. We will come to why gram matix is a good representation for style in a bit. Now, to transfer the source image towards a target style, the deep feature representation of the source image is inverted back to an image using the backpropagation method[^UnderstandDeep] with an additional style criteria that minimizes the L2 distance between the gram metrices of the target style and the gram metrices of the stylized source image. Without the style loss, the invertion would lead to a faithful reconstruction of the input source image. With the style loss, the invertion is driven towards the style in the target image.

There are some non-trivial observations in this work and some followup works: 
1. Deep parametric representation with simple statistics works really well for certain textures. The heart of Gatys et al.'s work[^NeuralStyleGatys] is the gram matrix, which can be proved for capturing Maximum Mean Discrepancy (MMD) with the second order polynomial kernel[^DemystifyingNeuralStyle]. However, using the gram matrix is not mandentary. The essence of neural style transfer is to match the feature distributions between the style images and the generated images. In fact, several other distribution alignment methods, including convariance matrix[^WCT], can also achive appealing results.
2. It is also interesting to see that simply inverting deep feature with both the content loss and style loss often lead to satisfactory results. Considering this: there is literaturelly no garrantee that the inverted image has plausible appearance, except for it needs to satisfy some constraint made in the deep latent space. The fact this works well indicates that the backpropagtion process must has some strong regularization that prevent the generated image from going crasy. This is very likely to due to the transposed convolution operation which act upon the image statistic encoded in the VGG network.
3. One catch is that it works a lot better is second order optmization method (L-BFGS[^LBFGS] in this partiular case) is used for updating the result. The results of first order optmization methods (SGD, momentum SGD, or Adam) seem to easily stuck in local optimal. In the contrast, L-BFGS does not only arrive the convergence a lot faster, buch also gives better results. This suggests that second order optmization methods do have certain advantage if the excat energy landscape can be fit into the memory. In this case, the optimization only concerns a single image so the target objective is excat.
4. The original L2 Norm used in Gatys et al. were replaced by the Earth Mover Distance in a recent work[^OptimalTransportNeuralStyle]. The author found its value in more accurately measuring the distance between features, which led to better style transfer. Histogram matchig[^HistogramNeuralStyle] is also invented to solve a particular artifact (under-saturated regions) in the transferred resutls.
5. It is not clear that aligning the distribution towards the style reference image is always a good idea, especially in cases where simple statistics cannot accurately approximate the distribution of the features. For this reason, style decomposition[^StyleDecomposition] is proposed to better model the subtle style differences within the same style image and then use the most appropriate sub-style (or mixtures of sub-style).


__Speed__: One of the directions orthogonal to image quality is speed. The original neural style[^NeuralStyleGatys] needs hundred of interations to converge. Putting this in aspect, it takes approximately to 1-2 mintutes for a Titan XP card to stylize a 1024x720 size image. This put the method firmly at the low speed category and makes it infeasible for applications such as realtime style transfer for videos.

The most time consuming part of neural style[^NeuralStyleGatys] is the iterative optmization step. To speed up the technique we need to replace this part by something faster. One option is using a feedforwad autoencoder to approximate the iteractive optimization. Intuitively, the autoencoder learns a nonlinear mapping from source images (in most case, realworld photos) to the look and feel of a target style (usually, artworks). The encoder part of the networks extract the semantic of the source image, and the decoder part of the network re-render the semantic with a new appearance (texture). To train such a network, the exactly the same content loss and style loss from [^NeuralStyleGatys] are used. Seminal works in this category J. Johnson et al's feedforward networks trained with perceptual loss[^PerceptualFastNeuralStyle], and D. Ulyanov et al's Texture Networks[^TextureNets]. The results are further improved by instance normalization[^InstanceNorm], which replace the normalization with batch statistics by normalization with image statistics. The insight of instance normalization is such that because there is a fixed style target (the distribtion of the features from the reference style image) for all inputs, it is helpful to normalize each input feature activation independently to a fixed input distribution (substract the mean of individual image, and scale by its standard deviation). Then learning a mapping from the fixed input distribution to a fixed target style distribution is easy. In contrast, batch normalization will first map each image to a different distribution based on the batch satistics, which makes the transfer to the fixed style target distribution a lot harder, and leads to sub-optimal results (weak style, diffused texture etc.).

__Versatility__ Another direction that is orthogonal to both quality and speed is versatility. Precisely, how universal is the method to cope with different target styles. To this, the pre-mentioned feedforward methods[^PerceptualFastNeuralStyle],[^TextureNets] are limited to a single target style. More precisely, the networks must be retrained for each style, make it expensive to train and store in practice.

Making the technqiue more versatile is hence of pyramount importance. Wouldn't it be nice to have one network that is capable of multiple styles, or even all posible styles?
To this end, there have been many exciting work[^AdaINNeuralStyle],[^SuperCharge],[^StyleBank], among which we will discuss the "whitening color transformation" method(WTC[^WCT]) in details due to its supereme versatility and mathmatical elegancy. The idea turns out to be very similar to the color image transfer[^ColorTransferBetweenImages], which first decorrelate the pixel value in color space and then apply a global translation and scale operation to align the per-channel color distribution of the source image toward to the target image. In the case of WCT, there is also a feature decorrelation process (whitening operation) and transform process (coloring operation). The difference is these operations are applied in deep feature space, so covariance matrix is used to performan the whitening and the coloring operation, as opposed to the pre-computed linear mapping between the RGB and $$l\alpha\beta$$ space. Formally, below are the whitening transformation and coloring transformation:

$$ \hat{f}_{c} = E_{c}D_{c}^{-\frac{1}{2}}E_{c}^{T}f_{c}, \ \ \ \hat{f_{cs}} = E{s}D_{s}^{\frac{1}{2}}E_{s}^{T}\hat{f_{c}}$$


The left equation is the whitening operation: $$D_{c}$$ is a diagonal matrix with the eigenvalues of the covariance matrix $$f_{c}f_{c}^{T} \in\Re^{C \times C}$$, and $$E_{c}$$ is the corresponding orthogonal matrix of eigenvectors, satisfying $$f_{c}f_{c}^{T} = E_{c}D_{c}E_{c}^{T}$$. The whitened feature map satisfies $$\hat{f_{C}}\hat{f_{C}^{T}} = I$$.

The right equation is the coloring operation: where $$D_{s}$$ is a diagonal matrix with the eigenvalues of the covariance matix $$f_{s}f_{s}^{T} \in \Re^{C \times C}$$, and $$E_{s}$$ is the corresponding orthogonal matrix of eigenvalues. The coloring operation satisfies $$\hat{f_{cs}}\hat{f_{cs}}^{T} = f_{s}f_{s}^{T}$$. Finally the colored feature map $$\hat{f_{cs}}$$ is re-centerred with the mean of the style features: $$\hat{f_{cs}} = \hat{f_{cs}} + m_{s}$$. 

Overall, the fact that the coloring operation satisfies $$\hat{f_{cs}}\hat{f_{cs}}^{T} = f_{s}f_{s}^{T}$$ means the distribution of transferred features match the distribution of target style features from the perspective of how whitened features correlated. In practice, this turns out to be a very good way for transfering style. The technique is also universal to arbitary styles: there is no need to train for each style, only a general autoencoder(trained with image reconstruction loss) is needed to compute the deep features. Then style transfer is as simple as decompose the covariance matrix of the features of the style image.

WCT is able to hit a good balance among quality, speed and versatility. In a recent followup work, the method is further improved from the speed aspect by replacing the relatively more expensive matrix decomposition by a neural network approximation[^FastWCT]. 


Before we move to the Non-Parametric method in the next chapter, it is worth mentioning that parametric method never worked well in the classical texture synthesis methods. Due to its lack of experss power in pixel space, and frequently distortion. This, however, is not a big problem for style transfer in deep feature space. One interesting perspective for why this is the case is features in high-dimensional space (after nonlinear transformed) are more easier to be modeled by simple staitistics. For example, distributions of different classes of objects are more linearly separable in deep feature space. It is interesting to see that such property really helped parametric methods to shine in the field of style transfer. 


__Non-parametric Neural Style Networks__ One natural extension of the classific non-parametric texture transfer method is to apply it with deep features. This stream of methods are a good complementary to the previously discussed parametric methods in the sense of both strength and weakness.

The first non-parametric neural style transfer is the CNNMRF[^CNNMRF] method proposed by C. Li et al. It is a straight forward extension of the classical non-parametric texture synthesis methods to the deep neural networks domain. Instead of matching and blending image patches in the pixel domain[^NonParametricSampling], CNNMRF recognizes that deep feature maps in neural networks can also be modeled as Markov Random Fields, where a neural activation only depend on its surrounding neighours. Doing so in the deep feature level allows semantic-based patch matching, which is way more robust than the pixels-based patch matching. At the sametime, blending "Neural Patches" of deep features and invert the blend result to an image is better than directly patch blending or stitching in the pixel domain, thanks to the rich image statistics encoded in the deep neural networks. In general, CNNMRF can handle more challenging texture transfer than convientional pixel-based methods. Compared to optimization based non-parametric Neural style[^NeuralStyleGatys], CNNMRF produce better results in transfering miso-structure from the target style, for example, shapes and photorealistic textures. On the other hand, it lack of the flexibility for paintly textures such as brush strokes and require stronger similarity between the source image and target image to get a successful transfer. Another interesting extension of classical non-parametric methods is the Deep image analogies method[^DeepAnalogy], which imploys a stricter constaint during feature matching and transfer. In CNNMRF, the neural patches are matched and blended independently at different layers, which has led to artifacts at regions that contains inconsistent matching as different layers. In particular,correspondence between two adjacent layers cannot be exactly maintained due to non-linear modules in CNNs (i.e., ReLU, Max-Pooling). To address this problem, deep image analogies transfer the texture layer by layer: it first transfer the feature maps at a higher layer before deconvolving the warped features for the next layer. In this way, texture transfer in the lower layer is guided by the results in the uppper layer. 


Efforts are also devoted to making non-parametric method faster for realtime performance. Similar to the fast non-parametric method, a feedforward network can be trained to approximate the mapping from input source image to the output stylized image[^FastPatch]. 


In general, parametric and non-parametric methods complement each other. However, non-parametric methods are more effecitive in classical pixel space due to , and parametric methods are more popular in deep learning time due to the fact deep features are more powerful and easier to be modeled with simple statistics. 


__Generative Adversersial Networks__

Both parametric and non-parametric style transfer have produced some amazing results. However, there are still many questions unanswered. 

The first thing that is questionable is the distance metric that is used to optimize the similarity between the style of the source and target images. Parametric methods use global statistics so tend to be too flexible, hence results often have distorted content and texture -- this is not a big problem for transfering paintly textures, but can harm the results of transfering more regular or photorealistic textures. On the other hand, non-parametric methods relies on neuralpatch matching (L2 Norm or correlation) hence tend to be too rigid. This causes Non-parametric methods only works for transfering style between images pairs that have high semantic similarity. 

What is a better metric that is neithe too flexible nor too rigid, so work well on both non-photorealistic and photorealistic style transfer? It there a way to learn a good metric for comparing the style similarity?

Anther question is none of the previous method can learne style at scale. For example, what is the common visual elements in Monet's painting that make them so unique. In other words, what make Monet's paintings look like Monet's painting? These methods compute the style representation using a pre-define function such as gram matrix, or using direct sampling of neural patches. To push the learning towards a collection of artworks from the same artists, or even a collection of artist of similar styles, it is necessary to look a more generalizable model for style.

To be clear, these are tough problems and far from being solved. But the research community have made significant progress with a piece of new learning algorithm called "Generative Adversersial Networks", or GANs. And in this chapter we will talk about the lastest progress in this field.

GANs[^GAN] is a method that lerans two things: First, it learns a distribution from discrete samples; Second, it learns a way to sample new data from the learnt distribution. To do so, GANs use two networks: First, a discriminator for classifying whether a sample comes from the target distribution or not. This is corresponding to the learning of distribution. Second, a generator that produce new, unseen samples from the learnt distribution. This is corresponding to the learning of how to sample. These two networks are learnt jointly with a min-max game, where the discriminator learns to classify real data from the generated data, wherase the genrator tries to produce data that fool the discriminator. The goal is, by the end of the learning, the generator is so good that the discrminator can no longer distinguish between which data are produced by the generator and which are the real sample. The cool thing about the generator lies in the way how it works: given an input distribution that is easy to sample data from, like uniform distribution or normal distribution, the generation is able to nonlinearly map the input distribution to an arbitarily complex target distribution. For example, the distribution of human face images.

So why is GANs important to style transfer? Well, the generation of images with a particular style can be treated as the process of sampling from a target distribution. The implication of GANs in the context of style transfer is that there is no need to parameterize this distribution using statistics like gram matrix or covariant matrix, and there is no need to stick to rigid copy of the example data like neural patches. The leant generate is able to approximate the process of sampling from that distribution. 

Precisely, to learn style of a single painting, or the common style of a collection of artworks, or a common style of a collection of artists, we simply treat these examples (patches in the painting in the first case, or individual artwork in the other two cases) as discrete samples from the target distribution. The generator in GANs is able to generate new patches or new artworks that are valid samples from that distribution. The discriminator acts as an universal solution for learning the metric for comparing the feature distirbution of the stylized images and the target images. No need to handcraft a similarity metrix anymore!

There is one more trick to make it work for style transfer: instead of taking random input, the generator takes the source image as the input. In this way, the generator learns to map feature distribution from the source image to the feature distribution of the target style. Another way to think about it is to make the generator conditioned on the input features.

One of the earliest work to apply GAN in paintly style transfer is the C. Li et al's Markovian GANs[^MGAN]. The authors first proofed using the discriminator in GANs can replace the Gram matrix for optmization based style transfer. They then train a generator (feed forward autoencoder) joinly with the discriminator for fast neural style transfer. Similarly, CycleGANs[^CycleGANs] use GANs on image patches and can amazingly transfer both photorealistic an non-photorealistic styles. To do so, a collection of related images are used during training for better generalization performance. For example, paintings from the same artist, or landscape photos took in the same season. The authors also invented the cycle loss to address the under-constrained in training GANs with unpaired data. In a followup work[^P2PHD], the other improve the conditional GANs for transfering photorealistic texture to semantic labeling maps or contour images, and allow interactive user editing. 

In general, GANs has a clear advantage in transfering photoreaslistic texture in comparison with the non-parametric and parametric methods. It is easy to see that the generator in GANs is very similar to the autoencoders in the previous methods. So the gain must come from somewhere else. One important reason could be that the adversaria learning of discrminator avoids direct style copying and handcrafting style representations. Another possible reason is the ability to learn style from a collection of images (as opposed to learning from a single image) allows the generator to generalize better.


#### Summary

This article gives an overview of the past and current progress in the field of styel transfer. We first review the landmark works in the pre deep learning literature and drawn conclusion the pros and cons of different techniques, including colour image transfer, emulation based non-photorealistic rendering, image filtering, and texture synthesis/transfer. We highlight the key insights of these techniques, for example, feature decorrelation and re-normalization for colour transfer, handcrafted filters for cartoon-like effect, markovian random field for modeling texture et al. We also discussed the limitation of these classical methods mainly comes from the low level feature representation and all the higher level model assumptions built upon it. 

With such knowledge, we move on to the recent development of deep learning based style transfer techniques. We first discuss using feature learnt from deep neural networks to improve the classical parametric and non-parametric style transfer methods. For example, fit staistiscal distribution to deep features is a very effective way to model painterly texture; whitening and coloring deep features enables a close-form solution for universal style transfer; Markovian Random Fields can be applied in deep feature maps to improve the results of non-parametric texture transfer. 

Last but not least, we highlight the implication of generaive adversarail networks in style transfer, and conclude its advantage in modeling complex and photo-realistic textures. We believe GANs' unmatched ability of learning from unpaired data and avoiding handcrafting model representation/learning metric make it a very powerful tool for developing novel style transfer techniques.  


#### Reference

[^TheStarryNight]: Painted by the Dutch [post-impressionist](https://en.wikipedia.org/wiki/Post-Impressionism) painter Vincent van Gogh, [The Starry Night](https://en.wikipedia.org/wiki/The_Starry_Night) is one of the most recognized paintings in the history of Western culture. Painted in June 1889, it describes the view from the east-facing window of his asylum room at Saint-Rémy-de-Provence, just before sunrise, with the addition of an ideal village. ![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg)
[^No5]: [No. 5, 1948](https://en.wikipedia.org/wiki/No._5,_1948) is a painting by Jackson Pollock, an American painter known for his contributions to the abstract expressionist movement. It was sold in May 2006 for $140 million, a new mark for highest ever price for a painting, not surpassed until April 2011. On inspection it was grey, brown, white and yellow paint drizzled in a way that many people still perceive as a "dense bird’s nest". ![alt text](https://images2.minutemediacdn.com/image/upload/c_crop,h_359,w_640,x_0,y_35/f_auto,q_auto,w_1100/v1555390051/shape/mentalfloss/jhk4jh45jk.png)
[^DeepPhotoStyleTransfer]: [Deep Photo Style Transfer](https://www.cs.cornell.edu/~fujun/files/style-cvpr17/style-cvpr17.html) Proposed by Luan et al, the method can transfer photorealistic style from one image to another. The main contribution is to constraint the  transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom fully differentiable energy term. ![alt text](https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2017/03/Photo-Style-Transfer-hed-796x398.jpg)
[^EverybodyDanceNow]: [Everybody Dance Now](https://carolineec.github.io/everybody_dance_now/) Proposed by Chan et al, this technique is capable of "do as I do" motion transfer: given a source video of a person dancing it can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. ![alt text](https://carolineec.github.io/everybody_dance_now/images/teaser.png)
[^ImageProcessing]: [Image processing](https://en.wikipedia.org/wiki/Digital_image_processing) is the method of using algorithms to process digital images. It is a fundamental field for low level computer vision, and involves techniques such as sampling, filtering, transforming, compression et al. Below shows the result of Canny Edge Detection -- a very important method for extracting edge information for downstream vision tasks. ![alt text](https://upload.wikimedia.org/wikipedia/commons/2/20/%C3%84%C3%A4retuvastuse_n%C3%A4ide.png)
[^FeatureDetection]: [Feature detection](https://en.wikipedia.org/wiki/Feature_detection_(computer_vision)) includes methods for extracing useful features from images. Common features include edges, corners/interest points, blobs/interest regions etc. Below is an example of Harris corner detector. It uses the second-moment matrix of the pixel values to summerize the predominant directions of the gradient in a specific pixel's neighbourhood. The eigenvalue of the matrix is used to calculate the corner response. Non-maximum supperssion is used to pick up the most predominant pixel as a corner from a local neighborhood. ![alt text](https://upload.wikimedia.org/wikipedia/commons/8/8d/Writing_Desk_with_Harris_Detector.png)
[^Segmentation]: [Contour Detection and Image Segmentation Resources](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html) from Computer Vision Group at UC Berkeley, a classical dataset for benchmarking image segmentation technique. Below is examples from the [Microsoft Common Objects in Context (MSCOCO)](http://cocodataset.org/#home) datas, the current resource for large-scale object detection, segmentation andn captioning. ![alt text](http://cocodataset.org/images/detection-splash.png)
[^TextureSynthesis]: In image spacing, [Texture](https://en.wikipedia.org/wiki/Texture_synthesis) refers to every digital image composed of repeated elements. The below image shows textures along a spectrum going from regular to stochastic. Texture analysis and synthesis address the problem of extracting characteristics from an example texture, and creates new textured images of similar characteristics. ![alt text](https://upload.wikimedia.org/wikipedia/commons/0/02/Texture_spectrum.jpg)
[^ParametricTextureModel]: [A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients](https://www.cns.nyu.edu/pub/eero/portilla99-reprint.pdf) present a universal statistical model for texture images in the context of an overcomplete complex wavelet transform. The model is parameterized by a set of statistics computed on pairs of coefficients corresponding to basis functions at adjacent spatial locations, orientations, and scales. Below are examples of texture synthesized with this method. The top row are the reference texture (real images), the bottom row are the synthesized images (fake images) that look like the reference. ![alt text](https://www.researchgate.net/profile/Javier_Portilla2/publication/2441269/figure/fig9/AS:394721869484035@1471120435252/Synthesis-results-on-photographic-aperiodic-textures-See-caption-of-Fig-12.png)
[^NonParametricSampling]: [Texture Synthesis by Non-parametric sampling](https://people.eecs.berkeley.edu/~efros/research/EfrosLeung.html) from A. Efros. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures. The top left region in the image below is the example from where the rest of the image is syntheized. ![alt text](http://www-sop.inria.fr/reves/Basilic/2009/WLKT09/tsynex.jpg)
[^TextureOptimization]: [Texture Optimiztion for Example based Texture Synthesis](https://www.cc.gatech.edu/cpl/projects/textureoptimization/) present a novel technique for texture synthesis using optimization. The method define a Markov Random Field (MRF)-based similarity metric for measuring the quality of synthesized texture with respect to a given input sample. It allows us to formulate the synthesis problem as minimization of an energy function, which is optimized using an Expectation Maximization (EM)-like algorithm.
[^GraphCut]: [An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Vision](http://www.csd.uwo.ca/faculty/yuri/Abstracts/pami04-abs.shtml) by Boykov et al. [Code](https://vision.cs.uwaterloo.ca/code/) available.
[^Autoencoder]: An [autoencoder](https://en.wikipedia.org/wiki/Autoencoder)) is a neural network that learns to copy its input to its output. It has an internal (hidden) layer that describes a code used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the original input. ![alt text](https://upload.wikimedia.org/wikipedia/commons/3/37/Autoencoder_schema.png)
[^GenerativeModels]: [Wikipedia page for generative models](https://en.wikipedia.org/wiki/Generative_model). Given an observable variable X and a target variable Y, a generative model is a statistical model of the joint probability distribution on X × Y.
[^GAN]: [Wikipedia page for Generative Adversarial Network](https://en.wikipedia.org/wiki/Generative_adversarial_network) A generative adversarial network (GAN) is a class of machine learning systems invented by Ian Goodfellow et al in 2014. Two neural networks contest with each other in a game (in the sense of game theory, often but not always in the form of a zero-sum game). Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics.
[^ColorTransferBetweenImages]: [Color Transfer Between Images](https://www.cs.tau.ac.il/~turkel/imagepapers/ColorTransfer.pdf), E. Reinhard et al., 2001. ![alt text](/assets/color-transfer.png)
[^NeuralStyleGatys]: [Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf). L. Gatys et al. CVPR 2016. ![alt text](/assets/neural-style-gatys.png)
[^WCT]: [Universal Style Transfer via Feature Transforms](https://arxiv.org/abs/1705.08086). Y. Li et al.
[^STARNPR]: [State of the ‘Art’: A Taxonomy of Artistic Stylization Techniques for Images and Video](https://core.ac.uk/download/pdf/9550248.pdf). J Kyprianidis et al. TVCG 2013.
[^HairyBrushes]: [Hairy brushes](https://pdfs.semanticscholar.org/5a13/cb0fdd262b26ba0f62018f983cee48dfb4ff.pdf). S. Strassmann, Siggraph 1986. Paint brushes are modeled as a collection of bristles which evolve over the course of the stroke, leaving a realistic image of a sumi brush stroke. ![alt text](/assets/hairy-brush.png)
[^WaterColor]: [Computer-Generated Watercolor](https://grail.cs.washington.edu/projects/watercolor/). C. Curtis et al, Siggraph 97. Watercolor model is based on an ordered set of translucent glazes, which are created independently using a shallow-water fluid simulation. The method used a Kubelka-Munk compositing model for simulating the optical effect of the superimposed glazes. ![alt text](/assets/water-color.png)
[^FastPaintTexture]: [Fast paint texture](http://www.dgp.toronto.edu/papers/ahertzmann_NPAR2002.pdf). A. Hertzmann, NPAR 2002. A technique for simulating the physical appearance of paint strokes under lighting. This technique is easyto-implement and very fast, yet produces realistic results. The system processes a painting composed of a list of brush strokes. A height map is assigned to each stroke, and a height field for the painting is produced by rendering the brush strokes textured with the height maps. The final painting is rendered by bump-mapping the painting’s colors with the height map. The entire process takes only a few seconds on current hardware. ![alt text](/assets/fast-paintly-texture.png)
[^CurvedBrushStroke]: [Painterly Rendering with Curved Brush Strokes of Multiple Sizes](https://www.mrl.nyu.edu/publications/painterly98/hertzmann-siggraph98.pdf) A method for creating an image with a handpainted appearance from a photograph, and a new approach to designing styles of illustration. The method “paint” an image with a series of spline brush strokes. Brush strokes are chosen to match colors in a source image. A painting is built up in a series of layers, starting with a rough sketch drawn with a large brush. The sketch is painted over with progressively smaller brushes, but only in areas where the sketch differs from the blurred source image. Thus, visual emphasis in the painting corresponds roughly to the spatial energy present in the source image. ![alt text](/assets/hertzmann-paintly.png)
[^Impressionist]: [Processing images and video for an impressionist effect](http://www.cs.virginia.edu/~dbrogan/CS551.851.animation.sp.2000/Papers/p407-litwinowicz.pdf) A technique that transforms ordinary video segments into animations that have a hand-painted look. The method is the first to exploit temporal coherence in video clips to design an automatic filter with a hand-drawn animation quality, in this case, one that produces an Impressionist effect. ![alt text](/assets/impressionist.png)
[^PenInk]: [Interactive pen-and-ink illustration](https://web.cs.ucdavis.edu/~ma/SIGGRAPH02/course23/notes/papers/Salisbury94.pdf). M. Salisbury. Siggraph 2002. This technique presented an interactive system for creating pen-and-ink illustrations. The system uses stroke textures—collections of strokes arranged in different patterns—to generate texture and tone. The user “paints” with a desired stroke texture to achieve a desired tone, and the computer draws all of the individual strokes. The image below using a grey scale image for reference. Left to right: Original grey scale image; extracted edges; curved hatching across the gradient.![alt text](/assets/pen-ink.png)
[^PaintByNumber]: [Paint by numbers: abstract image representations](https://dl.acm.org/citation.cfm?id=97902) This technique showed that it is possible to create abstract images using an ordered collection of brush strokes. These abstract images filter and refine visual information before it is presented to the viewer. By controlling the color, shape, size, and orientation of individual brush strokes, impressionistic paintings of computer generated or photographic images can easily be created.![alt text](/assets/paint-by-numbers.png)
[^VideoAbstracion]: [Real-time video abstraction](http://holgerweb.net/PhD/Research/papers/videoabstraction.pdf). H. Winnemöller et al. Siggraph 2006. This technique present an automatic, real-time video and image abstraction framework that abstracts imagery by modifying the contrast of visually important features, namely luminance and color opponency. We reduce contrast in low-contrast regions using an approximation to anisotropic diffusion, and artificially increase contrast in higher contrast regions with difference-of-Gaussian edges. The abstraction step is extensible and allows for artistic or data-driven control.![alt text](/assets/video-abstraction.png)
[^MeanShift]: [Mean shift: A robust approach toward feature space analysis](https://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf) A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data,  the method prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. Below shows the application of the algorithm in image segmentation ![alt text](/assets/meanshift.png)
[^StylizationDeCarlo]: [Stylization and Abstraction of Photographs](artis.imag.fr/Membres/Pascal.Barla/projet_image/sg02.pdf) The authors proposed a computational approach to stylizing and abstracting photographs that explicitly responds to this design goal. The system transforms images into a line-drawing style using bold edges and large regions of constant color. To do this, it represents images as a hierarchical structure of parts and boundaries computed using state-of-the-art computer vision. The system identifies the meaningful elements of this structure using a model of human perception and a record of a user’s eye movements in looking at the photo; the system renders a new image using transformations that preserve and highlight these visual elements. ![alt text](/assets/StylizationDeCarlo.png)
[^StrokeSurface]: [Stroke surfaces: Temporally coherent non-photorealistic animations from video](http://www.ee.surrey.ac.uk/CVSSP/Publications/papers/Collomosse-TVCG-2005.pdf) The below image is A visualization of the Stroke Surfaces generated from a video  sequence, which, when intersected by the a time plane, generate the region boundaries of a temporally coherent segmentation. The boundary of single video object, for example, the sheep’s body, may be described by multiple Stroke Surfaces. The image below compares radially symmetric kernel mean shift (left) and anisotropic kernel mean shift(right) on a spatio-temporal slice of the monkey bars sequence. The straighter edges in vertical (temporal) dimension lead to improved temporal coherence. ![alt text](/assets/stroke-surface.png)
[^VideoTooning]: [Video Tooning](https://www.juew.org/publication/VideoTooningFinal.pdf) A system is proposed for transforming an input video into a highly abstracted, spatio-temporally coherent cartoon animation with a range of styles. To achieve this, the system treats video as a space-time volume of image data. An anisotropic kernel mean shift technique is developed to segment the video data into contiguous volumes. These provide a simple cartoon style in themselves, but more importantly provide the capability to semi-automatically rotoscope semantically meaningful regions. The image below compares radially symmetric kernel mean shift (left) and anisotropic kernel mean shift(right) on a spatio-temporal slice of the monkey bars sequence. The straighter edges in vertical (temporal) dimension lead to improved temporal coherence. ![alt text](/assets/video-tooning.png)
[^ArtyShape]: [Arty Shapes](https://www.eecs.qmul.ac.uk/~yzs/files/Songetal_cae08.pdf) The technique shows that shape simplification is a tool useful in Non-Photorealistic rendering from photographs, because it permits a level of abstraction otherwise unreachable. A variety of simple shapes (e.g. circles, triangles, squares, superellipses and so on) are optimally fitted to each region within a segmented photograph. The system automatically chooses the shape that best represents the region; the choice is made via a supervised classifier so the “best shape” depends on the subjectivity of a user. ![alt text](/assets/artyshapes.png)
[^Relaxation]: [Paint By Relaxation](https://pdfs.semanticscholar.org/1c84/e1776aa6af16354077d92a7cb5b0184381dc.pdf) The technique use relaxation to produce painted imagery from images and video. An energy function is first specified; we then search for a painting with minimal energy. The appeal of this strategy is that, ideally, we need only specify what we want, not how to directly compute it. Because the energy function is very difficult to optimize, the technique use a relaxation algorithm combined with search heuristics. The image below greedy paintly rendering method of with 1 and 2, layers, respectively. The algorithm has difficulty capturing detail below the stroke size. Bottom: Paint by relaxation, with 1 and 2 layers. Strokes are precisely aligned to image features, especially near sharp contours, such as the lower-right corner of the jacket.![alt text](/assets/paint-by-relaxation.png)
[^ImageQuilting]: [Image Quilting](https://people.eecs.berkeley.edu/~efros/research/quilting/quilting.pdf). A. Efros et al. The technique first uses quilting as a fast and very simple texture synthesis algorithm which produces surprisingly good results for a wide range of textures. Second, the technique extends the algorithm to perform texture transfer – rendering an object with a texture taken from a different object. More generally, the technique demonstrate how an image can be re-rendered in the style of a different image. The method works directly on the images and does not require 3D information. The image below highlights the pixels and image patches that excite the neuron at one layer of a deep neural network.  ![alt text](https://upload.wikimedia.org/wikipedia/commons/b/bc/Imagequilting.gif)
[^ImageAnalogies]: [Image Analogies](https://mrl.nyu.edu/publications/image-analogies/analogies-72dpi.pdf). Aaron Hertzmann et al. The technique describes a new framework for processing images by example, called “image analogies.” The framework involves two stages: a design phase, in which a pair of images, with one image purported to be a “filtered” version of the other, is presented as “training data”; and an application phase, in which the learned filter is applied to some new target image in order to create an “analogous” filtered result. ![alt text](/assets/image-analogies.png)
[^VisualizeZeiler]: [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf). M. Zeiler et al. To examine a convnet, a deconvnet is attached to each of its layers, providing a continuous path back to image pixels. To start, an input image is presented to the convnet and features computed throughout the layers. To examine a given convnet activation, the method sets all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is reached. ![alt text](/assets/visualization.png)
[^UnderstandDeep]: [Understanding Deep Image Representations by Inverting Them](https://arxiv.org/abs/1412.0035). A. Mahendran et al. 2014. The authors conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. The image below shows the reconstruction using feature maps at different layers of a deep neural networks. ![alt text](/assets/understanding-reconstruction.png)
[^SynthesisPreferredInput]: [Synthesizing the preferred inputs for neurons inneural networks via deep generator networks](https://arxiv.org/abs/1605.09304). A. Nguyen et al. NIPS 2016. The authors dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method ![alt text](/assets/visualization-synthesis.png)
[^BuildingBlockInterpretability]: [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/). C Olah. Distill 2018. The method treats existing interpretability methods as fundamental and composable building blocks for rich user interfaces. We find that these disparate techniques now come together in a unified grammar, fulfilling complementary roles in the resulting interfaces. Moreover, this grammar allows us to systematically explore the space of interpretability interfaces, enabling us to evaluate whether they meet particular goals. The image below pair every neuron activation with a visualization of that neuron and sort them by the magnitude of the activation. This marriage of activations and feature visualization changes our relationship with the underlying mathematical object. ![alt text](/assets/visualization-buildingblocks.png)
[^DeepDream]: [Inceptionism: Going Deeper into Neural Networks ](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html). A. Mordvintsev. Google AI blo  2015. By choosing higher-level layers, which identify more sophisticated features in images, complex features or even whole objects tend to emerge. Just start with an existing image and give it to our neural net. We ask the network: “Whatever you see there, I want more of it!” This creates a feedback loop: if a cloud looks a little bit like a bird, the network will make it look more like a bird. This in turn will make the network recognize the bird even more strongly on the next pass and so forth, until a highly detailed bird appears, seemingly out of nowhere. ![alt text](/assets/deep-dream.png)
[^DemystifyingNeuralStyle]: [Demystifying neural style transfer](https://arxiv.orgabs/1701.01036). 
[^LBFGS]: [Limited-memory BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)
[^OptimalTransportNeuralStyle]: [Style Transfer by Relaxed Optimal Transport and Self-Similarity](https://arxiv.org/abs/1904.12785)
[^StyleDecomposition]: [Style Decomposition for Improved Neural Style Transfer](https://arxiv.org/abs/1811.12704)
[^HistogramNeuralStyle]: [Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses](https://arxiv.org/pdf/1701.08893.pdf)
[^PerceptualFastNeuralStyle]: [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)
[^TextureNets]: [ Texture Networks: Feed-forward Synthesis of Textures and Stylized Images](https://arxiv.org/abs/1603.03417).
[^InstanceNorm]: [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022).
[^AdaINNeuralStyle]: [Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization](https://arxiv.org/abs/1703.06868)
[^SuperCharge]: [A Learned Representation For Artistic Style](https://arxiv.org/abs/1610.07629)
[^StyleBank]: [StyleBank: An Explicit Representation for Neural Image Style Transfer](https://arxiv.org/abs/1703.09210)
[^FastWCT]: [Learning Linear Transformations for Fast Image and Video Style Transfer](http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Learning_Linear_Transformations_for_Fast_Image_and_Video_Style_Transfer_CVPR_2019_paper.html)
[^CNNMRF]: [Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis](https://arxiv.org/abs/1601.04589)
[^FastPatch]: [Fast Patch-based Style Transfer of Arbitrary Style](https://arxiv.org/abs/1612.04337)
[^DeepAnalogy]: [Visual Attribute Transfer through Deep Image Analogy](https://arxiv.org/abs/1705.01088)
[^MGAN]: [Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks](https://arxiv.org/abs/1604.04382)
[^CycleGANs]: [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://junyanz.github.io/CycleGAN/)
[^P2PHD]: [High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](https://arxiv.org/pdf/1711.11585.pdf)